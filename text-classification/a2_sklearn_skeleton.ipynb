{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following Python libraries are used for this part, and have been tested on Python 3.9 and Python 3.7.\n",
    " See links for instructions on installation if not already installed.\n",
    "  - [NLTK](https://www.nltk.org/install.html) (tested with 3.6.7 and with 3.2.5.)\n",
    "  - [Scikit-Learn](https://scikit-learn.org/stable/install.html) (test with 1.0.2)\n",
    "  - [SciPy](https://scipy.org/install/) (tested with 1.7.3 and with 1.4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You may uncomment and run the code cell below to download the data.  Otherwise, you may download the data [here](https://drive.google.com/file/d/1thWkUj7uGOApr_dXRvMr9TsEHpo_H_2q/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install gdown\n",
    "# !gdown --id 1thWkUj7uGOApr_dXRvMr9TsEHpo_H_2q -O sst2.zip\n",
    "# !mkdir -p data\n",
    "# !unzip sst2.zip -d data\n",
    "# !rm sst2.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 0. Building and Extracting features\n",
    " ### Build Vocabulary using __sst2.train__\n",
    " In the following cell, you will build a vocabulary to record all unique words (except the label) apearing in the training data. A vocabulary is a dictionary that maps a word to an unique integer. However, to better connect the scikit-learn model to the Pytorch Lightning model. We will need to add special tokens `<pad>` and `<unk>` to the vocabulary as the first two words. Basically, you will use NLTK WordPunctTokenizer to tokenize each row and lowercase every tokens. Then, you will aggregate tokens of all lines and then you can build your vocabulary dictionary.\n",
    "\n",
    " Save the vocab dictionary into a JSON file in the `data_dir` directory and name it `unigram_vocab.json` .\n",
    "\n",
    " Example vocabulary after processing a corpus `I have a cat.`:\n",
    "\n",
    " `{'<pad>': 0, '<unk>': 1, 'i': 2, 'have': 3, 'a': 4, 'cat': 5}`\n",
    "\n",
    " Note that each line of the file starts with the label âˆˆ (0, 1) followed by the text of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build unigram vocab from sst2.train\n",
      "Size of training data: 6920\n",
      "Vocab size before frequency filtering: 13850\n",
      "Vocab size after frequency filtering: 4949\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "print(\"Build unigram vocab from sst2.train\")\n",
    "data_dir = Path('data/')  # Modify the path of `data_dir` as needed.\n",
    "tokenizer = WordPunctTokenizer()\n",
    "counter = Counter()\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "data_train = open(data_dir.joinpath('sst2.train')).readlines()\n",
    "print(f\"Size of training data: {len(data_train)}\")\n",
    "\n",
    "# Hw-TODO: Update \"counter\" with occurances of words in the training data.\n",
    "for line in data_train: \n",
    "    l_tokens = tokenizer.tokenize(line.lower())\n",
    "    counter.update(l_tokens[1:])\n",
    "print(f\"Vocab size before frequency filtering: {len(counter)}\")\n",
    "\n",
    "vocab = {'<pad>': 0, '<unk>': 1}\n",
    "# HW TODO: Create the vocabulary with words that appear at least 3 times.\n",
    "id_ = 2\n",
    "for token, count in counter.items():\n",
    "    if count >= 3:\n",
    "        vocab[token] = id_\n",
    "        id_ += 1\n",
    "\n",
    "print(f\"Vocab size after frequency filtering: {len(vocab)}\")\n",
    "output_filepath = data_dir.joinpath('unigram_vocab.json')\n",
    "json.dump(vocab, open(output_filepath, mode='w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert (vocab['<pad>'] == 0)\n",
    "assert (vocab['<unk>'] == 1)\n",
    "assert (len(vocab) == 4949)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hw-TODO: Create vocabularies using two new feature templates. \n",
    "#          This corresponds to Part 2 of the assignment \"Feature Engineering\".\n",
    "#          You may come back to this after going through the rest of Part 1.\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Generate features and labels files\n",
    " In this part, you will convert each input text into a unigram binary representation. When process a line, you will first separate the label and the input text. Then, you will create a list filled with zeros of length that equals to the size of vocabulary (number of unique words including special tokens; feel free to explore sparse representation on your own). Remeber to lowercase the text and use WordPunctTokenizer for tokenization.\n",
    "\n",
    " In the end, for each data split, you will have a feature matrix $M$ with shape $(N, \\text{vocab\\_size})$ where $N$ is dataset size. Each column represents a word. For example, $M_{ij} \\in \\{0,1\\}$ denotes whether $i$-th instance has the word with id $j$ from the vocabulary. Also, you will have a label array with shape $(N,)$.\n",
    "\n",
    " When you do feature engineering, keep the naming system consistant so that your feature files can be applied to Pytorch-Lightning code easily. For example, you may save the features with the filename `{train,dev,test}_unigram_binary_features.npz` and the labels with the filename `{train,dev,test}_labels.npz` in the `data_dir` directory. \n",
    " And when you extract other features, the filename of the feature can be `{train,dev,test}_{FEATURE_NAME}_features.npz`.\n",
    "\n",
    " To reduce the storage of saving a whole dataset into a dense matrix, we highly recommend you to store features into a sparse matrix using `scipy.sparse.csr_matrix` ([api](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)). Otherwise, it takes you several GBs for this assignment. See a tutorial [here](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/). For the labels, you can store them as vanilla numpy arrays. (You may use `numpy.asarray()` to convert a Python `list` object to a `numpy.ndarray`.)\n",
    "\n",
    " To save the labels (as a `numpy.ndarray`), use `numpy.savez(filepath, labels_array)` ([api](https://numpy.org/doc/stable/reference/generated/numpy.savez.html)), and to save the features (as a `scipy.sparse.csr_matrix`, use `scipy.sparse.save_npz(filepath, features_sparse_matrix)` ([api](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html)) if you use the sparse matrix for features.\n",
    "\n",
    " As a sanity check, your `unigram_binary` matrix shapes for {train, dev, test} should look like:\n",
    "\n",
    " train feature matrix shape: (6920, 4949)\n",
    " train label array shape: (6920,)\n",
    "\n",
    " dev feature matrix shape: (872, 4949)\n",
    " dev label array shape: (872,)\n",
    "\n",
    " test feature matrix shape: (1821, 4949)\n",
    " test label array shape: (1821,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate `npz` files of features and of labels\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def extract_features(vocab, data_dir, tokenizer, feature_name):\n",
    "    \"\"\"\n",
    "    Extract and save different features based on vocab of the features.\n",
    "    # Parameters\n",
    "    vocab : `dict[str, int]`, required.\n",
    "        A map from the word type to the index of the word.\n",
    "    data_dir : `Path`, required.\n",
    "        Directory of the dataset\n",
    "    tokenizer : `Callable`, required.\n",
    "        Tokenizer with a method `.tokenize` which returns list of tokens.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    # Returns\n",
    "        `None`\n",
    "    \"\"\"\n",
    "    if feature_name == 'unigram_binary': \n",
    "        for set_ in ['train', 'dev', 'test']:\n",
    "            data = open(data_dir.joinpath(f'sst2.{set_}')).readlines()\n",
    "            row = []\n",
    "            col = []\n",
    "            values = []\n",
    "            labels = []\n",
    "            shape = (len(data), len(vocab))\n",
    "            for i, line in enumerate(data): \n",
    "                l_tokens = tokenizer.tokenize(line)\n",
    "                labels.append(int(l_tokens[0]))\n",
    "                tokens = l_tokens[1:]\n",
    "                unk_flag = True\n",
    "                for token in tokens: \n",
    "                    if token in vocab: \n",
    "                        col.append(vocab[token])\n",
    "                        row.append(i)\n",
    "                        values.append(1)\n",
    "                    elif unk_flag:\n",
    "                        col.append(vocab['<unk>'])\n",
    "                        row.append(i)\n",
    "                        values.append(1)\n",
    "                        unk_flag = False\n",
    "            matrix = sparse.csr_matrix((values, (row, col)), shape=shape)\n",
    "            labels = np.array(labels)\n",
    "            label_path = data_dir.joinpath(f'{set_}_labels.npz')\n",
    "            feature_path = data_dir.joinpath(f'{set_}_{feature_name}_features.npz')\n",
    "            np.savez(label_path, labels)\n",
    "            sparse.save_npz(feature_path, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = Path('data')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "vocab_filepath = data_dir.joinpath('unigram_vocab.json')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When you implement the other feature templates you choose for Part 2 \"Feature Engineering\", you'll copy the few lines of code above, adjusting the parameters as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We provide you the helper function below for feature weight analysis (1.1.2 and 1.2.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_important_weights(weights, words):\n",
    "    \"\"\"\n",
    "    Print important pairs of weights and words.\n",
    "    # Parameters\n",
    "    weights : `Iterable`, required.\n",
    "        Weights from a learned model.\n",
    "    words : `Iterable`, required.\n",
    "        Word types of the vocabulary.  \n",
    "        It must be true that `len(weights) == len(words)`.\n",
    "    # Returns\n",
    "        `None`\n",
    "    \"\"\"\n",
    "\n",
    "    def print_pairs(pairs):\n",
    "        for weight, word in pairs:\n",
    "            print(\"{: .4f} | {}\".format(weight, word))\n",
    "\n",
    "    assert len(weights) == len(words)\n",
    "    pairs = list(zip(weights, words))\n",
    "    pairs = sorted(pairs, key=lambda x: x[0], reverse=True)\n",
    "    print(\"Most positive words:\")\n",
    "    print_pairs(pairs[:10])\n",
    "    print(\"\\nMost negative words:\")\n",
    "    print_pairs(reversed(pairs[-10:]))\n",
    "\n",
    "    pairs = list(zip(abs(weights), words))\n",
    "    pairs = sorted(pairs, key=lambda x: x[0], reverse=False)\n",
    "    print(\"\\nMost neutral words:\")\n",
    "    print_pairs(pairs[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Scikit-learn specific part: logistic regression with scikit-learn\n",
    " ## 1.1.1 Logistic regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def fit_and_eval_logistic_regression(data_dir: Path,\n",
    "                                     feature_name: str) -> LogisticRegression:\n",
    "    \"\"\"\n",
    "    Fit and evaluate the logistic regression model using the scikit-learn library.\n",
    "    # Parameters\n",
    "    data_dir : `Path`, required\n",
    "        The data directory.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    # Returns\n",
    "        model_trained: `LogisticRegression`\n",
    "            The object of `LogisticRegression` after it is trained.\n",
    "    \"\"\"\n",
    "    # Hw-TODO: Implement logistic regression with scikit-learn.\n",
    "    #          Print out the accuracy scores on dev and test data.\n",
    "    #          Feel free to add arguments to the functions as needed.\n",
    "\n",
    "    train_data = sparse.load_npz(data_dir.joinpath(f'train_{feature_name}_features.npz'))\n",
    "    dev_data = sparse.load_npz(data_dir.joinpath(f'dev_{feature_name}_features.npz'))\n",
    "    test_data = sparse.load_npz(data_dir.joinpath(f'test_{feature_name}_features.npz'))\n",
    "\n",
    "    train_labels = np.load(data_dir.joinpath(f'train_labels.npz'))['arr_0']\n",
    "    dev_labels = np.load(data_dir.joinpath(f'dev_labels.npz'))['arr_0']\n",
    "    test_labels = np.load(data_dir.joinpath(f'test_labels.npz'))['arr_0']\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0).fit(train_data, train_labels)\n",
    "    dev_pred = clf.predict(dev_data)\n",
    "    test_pred = clf.predict(test_data)\n",
    "\n",
    "    dev_accuracy =  accuracy_score(dev_labels, dev_pred)\n",
    "    dev_f1 = f1_score(dev_labels, dev_pred)\n",
    "    test_accuracy = accuracy_score(test_labels, test_pred)\n",
    "    test_f1 = f1_score(test_labels, test_pred)\n",
    "\n",
    "\n",
    "    print(f\"Dev accuracy: {round(dev_accuracy, 4)}, Dev f1: {round(dev_f1, 4)}\")\n",
    "    print(f\"Test accuracy: {round(test_accuracy, 4)}, Test f1: {round(test_f1, 4)}\")\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.7821, Dev f1: 0.7879\n",
      "Test accuracy: 0.804, Test f1: 0.8059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='unigram_binary',\n",
    "                                 data_dir=Path('data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1.2 Weights Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.7821, Dev f1: 0.7879\n",
      "Test accuracy: 0.804, Test f1: 0.8059\n",
      "Most positive words:\n",
      " 1.9463 | solid\n",
      " 1.9113 | powerful\n",
      " 1.8853 | remarkable\n",
      " 1.7473 | fun\n",
      " 1.7471 | refreshing\n",
      " 1.6852 | enjoyable\n",
      " 1.6273 | terrific\n",
      " 1.6078 | definitely\n",
      " 1.5995 | appealing\n",
      " 1.5932 | works\n",
      "\n",
      "Most negative words:\n",
      "-2.0515 | stupid\n",
      "-1.9932 | suffers\n",
      "-1.9469 | worst\n",
      "-1.9229 | mess\n",
      "-1.9169 | dull\n",
      "-1.8535 | unfortunately\n",
      "-1.7629 | lacking\n",
      "-1.6785 | bland\n",
      "-1.6536 | none\n",
      "-1.6491 | flat\n",
      "\n",
      "Most neutral words:\n",
      " 0.0000 | <pad>\n",
      " 0.0000 | prophecies\n",
      " 0.0001 | maintain\n",
      " 0.0001 | manners\n",
      " 0.0001 | hawn\n",
      " 0.0001 | misogyny\n",
      " 0.0002 | fanciful\n",
      " 0.0002 | filming\n",
      " 0.0003 | bollywood\n",
      " 0.0004 | situations\n"
     ]
    }
   ],
   "source": [
    "model_trained: LogisticRegression = fit_and_eval_logistic_regression(\n",
    "    feature_name='unigram_binary', data_dir=Path('data'))\n",
    "weights = model_trained.coef_[0]\n",
    "vocab = json.load(open(data_dir.joinpath('unigram_vocab.json')))\n",
    "print_important_weights(weights=weights, words=vocab.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When you implement the other feature templates you choose for Part 2 \"Feature Engineering\", you'll make the same two calls above, adjusting the parameters as needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
