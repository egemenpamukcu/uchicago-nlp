{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632c61a1",
   "metadata": {},
   "source": [
    "The following Python (version 3.8.5) libraries are needed for this part.   \n",
    "See links for instructions on installation if not already installed.\n",
    " - [scipy](https://scipy.org/install/) 1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb9aad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10787ac",
   "metadata": {},
   "source": [
    "## 2 Distributional Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511244c",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "Implement distributional counting as described above for a provided w, V , and V<sub>C</sub>. Submit your code. Using vocab-15kws.txt to populate V and vocab-5k.txt to populate V<sub>C</sub> , use your code to report #(x, y) for the pairs in the following table for both w = 3 and w = 6 on wiki-1percent.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ca65d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_file = 'dataset/wiki-1percent.txt'\n",
    "vocab_file = 'dataset/vocab-15kws.txt'\n",
    "vocab_context_file = 'dataset/vocab-5k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b3ecc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_embedding_matrix(corpus_file, window_size, vocab_file, vocab_context_file):\n",
    "    # TODO: Complete this function to create and store the word embeddings for both vocabulary files.\n",
    "    #       You may adjust arguments to the function as needed.\n",
    "    # INPUT:\n",
    "    #   corpus_file: string, path to the textual corpus\n",
    "    #   window_size: int\n",
    "    #   vocab_file: string, path to vocabulary file, V\n",
    "    #   vocab_context_file: string, path to the context vocabulary, V_C\n",
    "    # OUTPUT: Return word embedding matrix. This may be a dictionary nesting dictionaries.\n",
    "    \n",
    "    # Read the vocabulary file and assign each word a unique ID\n",
    "    # Read the context vocabulary file and assign each word a unique ID\n",
    "\n",
    "    # Create word embedding matrix. You may find it helpful to use a sparse data structure (e.g., dictionary) here.\n",
    "    # For example, you can use a sparse data structure to map a word (from V) to a sparse data structure \n",
    "    #  that maps context words (from V_C) to counts, or a dictionary nesting dictionaries.\n",
    "    # sparse matrices in scipy would work too\n",
    "    \n",
    "    # You may create additional helper functions as needed.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4360f0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_pair_counts(word_embed, window_size):\n",
    "    # TODO: complete this function\n",
    "    # INPUT:\n",
    "    #   word_embed: dictionary, the word embedding matrix created in create_word_embedding_matrix()\n",
    "    #   window_size: int\n",
    "    # OUTPUT: report and print the #(x,y) pairs for the A1 write-up table for window_size 3 and 6.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7433f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embed_3 = create_word_embedding_matrix(corpus_file=corpus_file,\n",
    "                                            window_size=3,\n",
    "                                            vocab_file=vocab_file,\n",
    "                                            vocab_context_file=vocab_context_file)\n",
    "report_pair_counts(word_embed_3, window_size=3)\n",
    "\n",
    "word_embed_6 = create_word_embedding_matrix(corpus_file=corpus_file,\n",
    "                                            window_size=6,\n",
    "                                            vocab_file=vocab_file,\n",
    "                                            vocab_context_file=vocab_context_file)\n",
    "report_pair_counts(word_embed_6, window_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb310e",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Using w = 3 (and again using vocab-15kws.txt for V and vocab-5k.txt for V<sub>C</sub>), evaluate your count-based word vectors by computing the Spearman correlations and report your results on MEN and SimLex-999. The function `eval_word_similarity` implements the EvalWS procedure. As a sanity check, your Spearman correlation for MEN should be close to 0.225.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ed4b0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    # TODO: complete this function to calculate cosine similarities between vectors a and b\n",
    "    # INPUT:\n",
    "    #   a, b: counter or dictionary, a mapping from each vocabulary token to its embedding.\n",
    "    # OUTPUT: return cosine similarity score\n",
    "    pass\n",
    "\n",
    "def eval_word_similarity(eval_file, word_embed):\n",
    "    # TODO: evaluate the similarity derived from the word embeddings\n",
    "    # INPUT:\n",
    "    #   eval_file: string, path to \"men.txt\" or \"simlex-999.txt\" files\n",
    "    #   word_embed: dictionary, this is the word embedding matrix created in create_word_embedding_matrix()\n",
    "    # OUTPUT: return Spearman's correlation score\n",
    "    # You are provided with files containing pairs of words along with their human-annotated similarity scores.\n",
    "    # You should use your word vectors to compute the cosine similarity of each word pair in the word similarity files.\n",
    "    # Use Spearman’s rank correlation coefficient (also called Spearman’s ρ; see scipy.stats.spearmanr) as the evaluation metric.\n",
    "    # That is, for each word pair in a word similarity dataset, you will compute its cosine similarity using your word vectors, which forms one list of similarities.\n",
    "    # Then, form another list of similarities for those same word pairs, this time containing the manually-annotated similarities.\n",
    "    # Compute Spearman’s ρ between the two lists of similarities.\n",
    "    # You should make use of the cos_sim() function that you completed above.\n",
    "    \n",
    "    # Change the return statement to return Spearman's correlation score\n",
    "    return math.nan\n",
    "\n",
    "\n",
    "def report_spearman_corr(eval_file, word_embed, window_size, embed_type):\n",
    "    score = eval_word_similarity(eval_file, word_embed)\n",
    "    print('dataset: {}, window size: {}, embedding_method: {}, spearmanr: {: .4f}'.format(\n",
    "        eval_file, window_size, embed_type, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa6b72",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Spearman corr for count embedding:')\n",
    "report_spearman_corr(eval_file='dataset/men.txt',\n",
    "                     word_embed=word_embed_3,\n",
    "                     window_size=3,\n",
    "                     embed_type='count')\n",
    "report_spearman_corr(eval_file='dataset/simlex-999.txt',\n",
    "                     word_embed=word_embed_3,\n",
    "                     window_size=3,\n",
    "                     embed_type='count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569199d",
   "metadata": {},
   "source": [
    "## 3 Combining Counts with Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8dfb6",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "Extend your implementation to be able to compute IDF-based word vectors using Eq. 1. Using w = 3, vocab-15kws.txt to populate V , and vocab-5k.txt to populate V<sub>C</sub> , evaluate your IDF-based word vectors and report your results with the Spearman correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551cdb42",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_idf_embedding_matrix(corpus_file, word_embed, vocab_context_file):\n",
    "    # TODO: complete this function create IDF-based embeddings\n",
    "    #       Combine counts with inverse document frequency (IDF)\n",
    "    #       Use the definition given in the assignment write-up to guide you.\n",
    "    #       You may adjust arguments to the function as needed.\n",
    "    # INPUT:\n",
    "    #   corpus_file: string, path to the textual corpus\n",
    "    #   word_embed: dictionary, the word embedding matrix created in create_word_embedding_matrix()\n",
    "    #   vocab_context_file: string, path to the context vocabulary, V_C\n",
    "    # OUTPUT: return an updated word embedding matrix incorporating IDF\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82832b57",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Calculate word_embed with IDF\\n\")\n",
    "idf_word_embed = create_idf_embedding_matrix(\n",
    "    corpus_file=corpus_file,\n",
    "    word_embed=word_embed_3,  # word embedding matrix with window size 3\n",
    "    vocab_context_file=vocab_context_file)\n",
    "\n",
    "print('Spearman corr for count embedding:')\n",
    "report_spearman_corr('dataset/men.txt',\n",
    "                     idf_word_embed,\n",
    "                     window_size=3,\n",
    "                     embed_type='idf')\n",
    "report_spearman_corr('dataset/simlex-999.txt',\n",
    "                     idf_word_embed,\n",
    "                     window_size=3,\n",
    "                     embed_type='idf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11ecde",
   "metadata": {},
   "source": [
    "## 4 Pointwise Mutual Information (PMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d1c0eb",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "Implement the capability of computing PMIs. Use your code to calculate PMIs for w = 3 when using vocab-15kws.txt to populate V and vocab-5k.txt to populate V<sub>C</sub>.\n",
    "For center word x = “coﬀee”, print the 10 context words with the largest PMIs and the 10 context words with the smallest PMIs. Print both the words and the PMI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4a878",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pmi_embedding_matrix(word_embed):\n",
    "    # TODO: Complete this function.\n",
    "    #       Calculate pointwise mutual information using formulas given in write-up\n",
    "    #       You may find it again useful here to use a sparse data structure (dictionary)\n",
    "    #       You may adjust arguments to the function as needed.\n",
    "    # INPUT:\n",
    "    #   word_embed: dictionary, the word embedding matrix created in create_word_embedding_matrix()\n",
    "    # OUTPUT: return the calculated PMIs as a dictionary\n",
    "    pass\n",
    "\n",
    "\n",
    "# The following function is provided. Given a center word, it outputs the 10 nearest and furtherest context words, by PMI.\n",
    "def report_nearest_farthest(word_embed, word, top=10):\n",
    "    if word_embed is None:\n",
    "        return\n",
    "\n",
    "    values = [(word_embed[word][c], c) for c in word_embed[word]]\n",
    "    values.sort()\n",
    "    print(\"Farthest:\")\n",
    "    for i in range(top):\n",
    "        score, word = values[i]\n",
    "        print(\"score: {: .5f}, word: {}\".format(score, word))\n",
    "    print(\"Nearest:\")\n",
    "    for i in range(top):\n",
    "        score, word = values[len(values) - 1 - i]\n",
    "        print(\"score: {: .5f}, word: {}\".format(score, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51563006",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Calculate word_embed with PMI\\n\")\n",
    "pmi_word_embed = create_pmi_embedding_matrix(word_embed_3)\n",
    "print(\"window size = {}\".format(3))\n",
    "report_nearest_farthest(pmi_word_embed, \"coffee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df00469",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "Now, define word vectors using PMI. That is, the word vector for a word x ∈ V has an entry for each word y ∈ V<sub>C</sub> with value given by pmi(x, y). As above, use w = 3, vocab-15kws.txt to populate V , and vocab-5k.txt to populate V<sub>C</sub>. Evaluate your PMI-based word vectors and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0276b2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Spearman corr for PMI embedding:')\n",
    "report_spearman_corr('dataset/men.txt',\n",
    "                     pmi_word_embed,\n",
    "                     window_size=3,\n",
    "                     embed_type='pmi')\n",
    "report_spearman_corr('dataset/simlex-999.txt',\n",
    "                     pmi_word_embed,\n",
    "                     window_size=3,\n",
    "                     embed_type='pmi')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e174f5",
   "metadata": {},
   "source": [
    "## 5 Quantitivate Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520e17c",
   "metadata": {},
   "source": [
    "### 5.1\n",
    "Evaluate the word vectors corresponding to the three ways of computing vectors (counts, IDF, and PMI), three values of w (1, 3, and 6), and two context vocabularies (vocab-15kws.txt and vocab-5k.txt). For all cases, use vocab-15kws.txt for V . Report the results (there should be 36 correlations in all) and describe your ﬁndings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754dd51b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in [1, 3, 6]:\n",
    "    print(f\"\\n***Evaluating with window size {w}***\\n\")\n",
    "    for vocab_context_file in ['dataset/vocab-15kws.txt', 'dataset/vocab-5k.txt']:\n",
    "        print(f\"\\n***Evaluating with context vocab {vocab_context_file}***\\n\")\n",
    "        word_embed = create_word_embedding_matrix(corpus_file=corpus_file,\n",
    "                                                  window_size=w,\n",
    "                                                  vocab_file='dataset/vocab-15kws.txt',\n",
    "                                                  vocab_context_file=vocab_context_file)\n",
    "        report_spearman_corr('dataset/men.txt',\n",
    "                             word_embed,\n",
    "                             window_size=w,\n",
    "                             embed_type='count')\n",
    "        report_spearman_corr('dataset/simlex-999.txt',\n",
    "                             word_embed,\n",
    "                             window_size=w,\n",
    "                             embed_type='count')\n",
    "\n",
    "        idf_word_embed = create_idf_embedding_matrix(\n",
    "            corpus_file=corpus_file,\n",
    "            word_embed=word_embed,\n",
    "            vocab_context_file=vocab_context_file)\n",
    "        report_spearman_corr('dataset/men.txt',\n",
    "                             idf_word_embed,\n",
    "                             window_size=w,\n",
    "                             embed_type='idf')\n",
    "        report_spearman_corr('dataset/simlex-999.txt',\n",
    "                             idf_word_embed,\n",
    "                             window_size=w,\n",
    "                             embed_type='idf')\n",
    "\n",
    "        pmi_word_embed = create_pmi_embedding_matrix(word_embed)\n",
    "        report_spearman_corr('dataset/men.txt',\n",
    "                             pmi_word_embed,\n",
    "                             window_size=w,\n",
    "                             embed_type='pmi')\n",
    "        report_spearman_corr('dataset/simlex-999.txt',\n",
    "                             pmi_word_embed,\n",
    "                             window_size=w,\n",
    "                             embed_type='pmi')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
