{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tX81sP7J-ta"
      },
      "source": [
        " The following Python libraries are required for this part, and have been tested on Python 3.9 and Python 3.7.\n",
        " If you use Google Colab, PyTorch is already installed.\n",
        "  - [PyTorch](https://pytorch.org/get-started/locally/) (tested with 1.10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcWCOlx1vSmf"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDlJ3yEpvX4n",
        "outputId": "911af6fe-06d3-4fcc-d43d-c7f871eb7ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# You may prefer to upload the data to your google drive and mount your google drive to this colab, \n",
        "# because the data will be erased if you stop using this colab for a while.\n",
        "# Uncomment the code below to do so. After mounting, navigate to the appropriate folder, right click, and \"copy path\".\n",
        "# Assign DATA_DIR global variable to that path.\n",
        "# For more mounting instructions: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=XDg9OBaYqRMd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "zyBV10nMJIrO"
      },
      "outputs": [],
      "source": [
        "# If imported from google drive, config for your file directory. Mine is 'lm_data'.\n",
        "DATA_DIR = \"/content/drive/MyDrive/nlp/a3/lm_data\"\n",
        "\n",
        "# the goal is that DATA_DIR points to where the training/validation/test data is. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "j2IKVqkxzN1L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "7asRBEtT102s"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "TRAIN_BATCH_SIZE = 100\n",
        "TEST_BATCH_SIZE = 100\n",
        "WORD_EMBED_DIM = 200\n",
        "HID_EMBED_DIM = 200 \n",
        "N_LAYERS = 2 \n",
        "DROPOUT = 0.5 \n",
        "LOG_INTERVAL = 100\n",
        "EPOCHS = 10\n",
        "BPTT = 50 # sequence length\n",
        "CLIP = 0.25\n",
        "TIED = False\n",
        "SAVE_BEST = os.path.join(DATA_DIR, 'model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_45k6t7hzgsA"
      },
      "source": [
        "## Build vocabulary and convert text in corpus to lists of word index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "xr3dkhUg2fiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f32ee9e-d2e6-432e-92d0-d12ab9e20854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2099444\n",
            "218808\n",
            "246993\n",
            "28913\n"
          ]
        }
      ],
      "source": [
        "class WordDict(object):\n",
        "    def __init__(self):\n",
        "        # mapping between word type to its index\n",
        "        self.word2idx = {'<sos>': 0, '<eos>': 1}\n",
        "        # mapping between index to word type\n",
        "        self.idx2word = ['<sos>', '<eos>']\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # TODO: add word to the dictionary by updating both word2idx and idx2word\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.train_file = os.path.join(path, 'train.txt')\n",
        "        self.valid_file = os.path.join(path, 'valid.txt')\n",
        "        self.test_file = os.path.join(path, 'test.txt')\n",
        "\n",
        "        self.dictionary = WordDict() \n",
        "\n",
        "        self.train = self.tokenize(self.train_file)\n",
        "        self.valid = self.tokenize(self.valid_file)\n",
        "        self.test = self.tokenize(self.test_file)\n",
        "                                   \n",
        "    def tokenize(self, filename):\n",
        "        ################################\n",
        "        ## TODO: \n",
        "        ## (1) build vocabulary on three given files, using class WordDict \n",
        "        ## (2) tokenize each file content with the vocabulary, return a list of token ids\n",
        "        ## Note that in this implementation, we add words in validation and test file into the vocabulary,\n",
        "        ## so there is no unknown word.\n",
        "        ################################ \n",
        "        \n",
        "        ids = []\n",
        "        with open(filename) as f:\n",
        "            for line in f: \n",
        "                if line.strip() != '': \n",
        "                    tokens = line.lower().split()\n",
        "                    for token in tokens:\n",
        "                        self.dictionary.add_word(token)\n",
        "                    tokens.append('<eos>')\n",
        "                    tokens.insert(0, '<sos>')\n",
        "                    ids += [self.dictionary.word2idx[token] for token in tokens]\n",
        "\n",
        "        return ids\n",
        "\n",
        "corpus = Corpus(DATA_DIR)\n",
        "print(len(corpus.train))\n",
        "print(len(corpus.valid))\n",
        "print(len(corpus.test))\n",
        "print(len(corpus.dictionary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "6of33pFT94dM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac73699-c612-409a-a20e-cd0577ee919f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20994, 100])\n",
            "torch.Size([2188, 100])\n",
            "torch.Size([2469, 100])\n"
          ]
        }
      ],
      "source": [
        "def batchify(ids, batch_size):\n",
        "    \"\"\"\n",
        "    batchify arranges the dataset into columns.\n",
        "    # Parameters\n",
        "    data : Tensor\n",
        "        1-dimensional tensor of token ids\n",
        "    batch_size : Int\n",
        "        batch_size\n",
        "    # Returns\n",
        "    data: a torch.LongTensor with shape of (batch_size, len(ids)//batch_size)\n",
        "        batchified corpus data\n",
        "\n",
        "    For example, the input ids [1,2,3,4,5,6,7,8,9] and batch_size=2\n",
        "    output is:\n",
        "    [ [1, 5],\n",
        "      [2, 6],\n",
        "      [3, 7],\n",
        "      [4, 8] ]\n",
        "    The shape of the tensor is 4x2. \n",
        "    We trim off any extra elements (9 in this example) that wouldn't cleanly fit.\n",
        "    ***Again, note that the text order is in the column.***\n",
        "    \"\"\" \n",
        "    ########\n",
        "    # TODO #\n",
        "    ########\n",
        "\n",
        "    dimension = len(ids) // batch_size\n",
        "    id_t = ids[:dimension * batch_size]\n",
        "    numpy_ids = np.array(id_t).reshape(batch_size, dimension)\n",
        "    \n",
        "    return torch.permute(torch.Tensor(numpy_ids), (1, 0))    \n",
        "\n",
        "train_data = batchify(corpus.train, TRAIN_BATCH_SIZE)\n",
        "val_data = batchify(corpus.valid, TEST_BATCH_SIZE)\n",
        "test_data = batchify(corpus.test, TEST_BATCH_SIZE)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "qhtQEbhR_hNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93aee8a-8fdb-4009-9e05-06f277662a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.8000e+01, 1.4000e+01, 6.2000e+01,  ..., 1.7113e+04, 2.0000e+00,\n",
            "         5.4070e+03],\n",
            "        [3.9000e+01, 3.5380e+03, 1.0000e+01,  ..., 2.6070e+03, 1.8169e+04,\n",
            "         3.8000e+01],\n",
            "        [4.0000e+01, 1.8620e+03, 1.8000e+01,  ..., 3.6000e+01, 1.2759e+04,\n",
            "         1.4018e+04],\n",
            "        ...,\n",
            "        [3.8000e+01, 3.4900e+03, 1.8000e+01,  ..., 1.6000e+01, 5.5000e+01,\n",
            "         6.2570e+03],\n",
            "        [6.1000e+01, 2.7500e+02, 5.5460e+03,  ..., 4.7200e+02, 1.5633e+04,\n",
            "         4.6910e+03],\n",
            "        [1.8000e+01, 1.0000e+01, 6.2000e+01,  ..., 6.0000e+01, 8.5430e+03,\n",
            "         1.7000e+01]])\n",
            "tensor([  39., 3538.,   10.,  ..., 1938.,   38., 1654.])\n"
          ]
        }
      ],
      "source": [
        "def get_batch(source, i):\n",
        "    \"\"\"\n",
        "    # Parameters\n",
        "    source : Tensor\n",
        "        corpus as 2-dimensional tensor\n",
        "    i : Int\n",
        "        minibatch index\n",
        "\n",
        "    # Returns\n",
        "    data : 2D tensor \n",
        "        LSTM input\n",
        "    target : 1D tensor\n",
        "        LSTM output target\n",
        "\n",
        "    Consider the following example where \"source\" is a 2d tensor of shape (4, 2).\n",
        "    In this example we have 4 batches, each of size 2.\n",
        "    [ [1, 5],\n",
        "      [2, 6],\n",
        "      [3, 7],\n",
        "      [4, 8] ]\n",
        "\n",
        "    Suppose we set BPTT (backpropagation through time, see A3 pdf for details) to 2.\n",
        "    At index i = 0, the input to our LSTM becomes:\n",
        "    [ [1, 5],\n",
        "      [2, 6] ]\n",
        "    This corresponds to the first 2 batches in the sequence.\n",
        "    The target would correspondingly be (again, since BPTT is 2): \n",
        "    [ [2, 6],\n",
        "      [3, 7] ]\n",
        "    However, we need to reshape it from 2-dimensions to 1-dimension:\n",
        "    [2, 6, 3, 7]\n",
        "\n",
        "    For the next batch, index i = prev_i + BPTT = 2. \n",
        "    However, i + BPTT = 2 + 2 = 4 and 4 >= len(source). This wouldn't work.\n",
        "    So, to account for this edge case, we consider BPTT to be:\n",
        "    len(source) - 1 - i = 4 - 1 - 2 = 1\n",
        "    As such, our input now becomes:\n",
        "    [ [3, 7] ]\n",
        "    and target is \n",
        "    [4, 8]. \n",
        "    \"\"\" \n",
        "    ###################################################\n",
        "    # TODO Assign these variables to the right values #\n",
        "    ###################################################\n",
        "\n",
        "    seq_len = BPTT\n",
        "    if i + BPTT >= len(source):\n",
        "        seq_len = len(source) - 1 - i\n",
        "\n",
        "    data = source[i : i + seq_len]\n",
        "    target = source[i + 1 : i + seq_len + 1].reshape(-1)\n",
        "    return data, target\n",
        "\n",
        "data, targets = get_batch(train_data, 50)\n",
        "print(data)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "6C8QNsUL9i1S"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "## TODO: Implement RNN LSTM\n",
        "## documentation of pytorch LSTM interface: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "################################\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, word_embedding_size, nhid, nlayers, dropout=0.5, tied_weights=False):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.nhid = nhid # hidden dimension of LSTM\n",
        "        self.nlayers = nlayers # number of LSTM layers\n",
        "        # TODO: initialize the required modules for the LSTM model\n",
        "        # HINT: batch_first should be False in LSTM since our data structure is not batch first.\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_embedding_size = word_embedding_size\n",
        "        self.encoder = nn.Embedding(vocab_size, word_embedding_size)\n",
        "        self.lstm = nn.LSTM(word_embedding_size, nhid, nlayers, batch_first=False)\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.decoder = nn.Linear(nhid, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        For example:\n",
        "        # initrange = 0.1\n",
        "        # nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        This is not all that you need!\n",
        "        \"\"\"\n",
        "        # TODO: initialize the parameters\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "        nn.init.uniform_(self.decoder.bias, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"\n",
        "        # Parameters\n",
        "        input: input embedding\n",
        "        hidden: hidden states in LSTM\n",
        "        # Returns\n",
        "        decoded: refers to the output of decoder layer over the vocabulary. Note that you don't need to pass it through the softmax layer\n",
        "        hidden: stores the hidden states in LSTM\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        embeddings = self.encoder(input.long())\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        lstm_out, hidden = self.lstm(embeddings, hidden)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        decoded = self.decoder(lstm_out.view(-1, self.word_embedding_size))\n",
        "\n",
        "        return decoded, hidden\n",
        "\n",
        "    # initialize parameters in LSTM\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "            weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "hVpQfjuxOm0Y"
      },
      "outputs": [],
      "source": [
        "# Set the random seed for reproducibility.\n",
        "torch.manual_seed(SEED)\n",
        "# set device as GPU/CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "id": "xZT5ZAu7EX3v"
      },
      "outputs": [],
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(TRAIN_BATCH_SIZE)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, BPTT)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        hidden = repackage_hidden(hidden) # Note that the main advantage here is that the hidden value is continual from the previous forward pass\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets.to(torch.int64))\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
        "            print(\"LOG INTERVAL\", LOG_INTERVAL)\n",
        "            cur_loss = total_loss / LOG_INTERVAL\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // BPTT,\n",
        "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "collapsed": true,
        "id": "aTsdN6rLG0fc"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute the loss of model on data_source\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    # TODO: get the average negative log likelihood on the data_source\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(TEST_BATCH_SIZE)\n",
        "    for batch, i in enumerate(range(0, data_source.size(0) - 1, BPTT)):\n",
        "        data, targets = get_batch(data_source, i)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        model.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden = model(data, hidden)\n",
        "\n",
        "        loss = criterion(output, targets.to(torch.int64))\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / (batch + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "collapsed": true,
        "id": "MYuaQ5cSHxWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1231d2-ac9b-45e3-b9cf-ed68a1f9eb17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOG INTERVAL 100\n",
            "| epoch   1 |   100/  419 batches | ms/batch 57.05 | loss  7.62 | ppl  2028.70\n",
            "LOG INTERVAL 100\n",
            "| epoch   1 |   200/  419 batches | ms/batch 56.57 | loss  6.82 | ppl   914.18\n",
            "LOG INTERVAL 100\n",
            "| epoch   1 |   300/  419 batches | ms/batch 56.36 | loss  6.56 | ppl   705.75\n",
            "LOG INTERVAL 100\n",
            "| epoch   1 |   400/  419 batches | ms/batch 56.34 | loss  6.43 | ppl   619.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 24.53s | valid loss  6.06 | valid ppl   429.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "LOG INTERVAL 100\n",
            "| epoch   2 |   100/  419 batches | ms/batch 56.85 | loss  6.37 | ppl   586.80\n",
            "LOG INTERVAL 100\n",
            "| epoch   2 |   200/  419 batches | ms/batch 56.28 | loss  6.23 | ppl   506.15\n",
            "LOG INTERVAL 100\n",
            "| epoch   2 |   300/  419 batches | ms/batch 56.29 | loss  6.16 | ppl   474.77\n",
            "LOG INTERVAL 100\n",
            "| epoch   2 |   400/  419 batches | ms/batch 56.36 | loss  6.11 | ppl   448.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 24.48s | valid loss  5.82 | valid ppl   335.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "LOG INTERVAL 100\n",
            "| epoch   3 |   100/  419 batches | ms/batch 56.85 | loss  6.11 | ppl   450.20\n",
            "LOG INTERVAL 100\n",
            "| epoch   3 |   200/  419 batches | ms/batch 56.30 | loss  5.99 | ppl   399.01\n",
            "LOG INTERVAL 100\n",
            "| epoch   3 |   300/  419 batches | ms/batch 56.32 | loss  5.94 | ppl   379.49\n",
            "LOG INTERVAL 100\n",
            "| epoch   3 |   400/  419 batches | ms/batch 56.34 | loss  5.90 | ppl   363.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 24.48s | valid loss  5.65 | valid ppl   285.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "LOG INTERVAL 100\n",
            "| epoch   4 |   100/  419 batches | ms/batch 56.75 | loss  5.92 | ppl   373.30\n",
            "LOG INTERVAL 100\n",
            "| epoch   4 |   200/  419 batches | ms/batch 56.30 | loss  5.82 | ppl   335.77\n",
            "LOG INTERVAL 100\n",
            "| epoch   4 |   300/  419 batches | ms/batch 56.27 | loss  5.78 | ppl   322.65\n",
            "LOG INTERVAL 100\n",
            "| epoch   4 |   400/  419 batches | ms/batch 56.25 | loss  5.74 | ppl   311.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 24.46s | valid loss  5.53 | valid ppl   252.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "LOG INTERVAL 100\n",
            "| epoch   5 |   100/  419 batches | ms/batch 56.77 | loss  5.78 | ppl   322.95\n",
            "LOG INTERVAL 100\n",
            "| epoch   5 |   200/  419 batches | ms/batch 56.38 | loss  5.68 | ppl   294.12\n",
            "LOG INTERVAL 100\n",
            "| epoch   5 |   300/  419 batches | ms/batch 56.32 | loss  5.65 | ppl   282.96\n",
            "LOG INTERVAL 100\n",
            "| epoch   5 |   400/  419 batches | ms/batch 56.26 | loss  5.62 | ppl   275.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 24.47s | valid loss  5.44 | valid ppl   231.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "LOG INTERVAL 100\n",
            "| epoch   6 |   100/  419 batches | ms/batch 56.79 | loss  5.66 | ppl   286.66\n",
            "LOG INTERVAL 100\n",
            "| epoch   6 |   200/  419 batches | ms/batch 56.43 | loss  5.57 | ppl   263.46\n",
            "LOG INTERVAL 100\n",
            "| epoch   6 |   300/  419 batches | ms/batch 56.25 | loss  5.54 | ppl   254.97\n",
            "LOG INTERVAL 100\n",
            "| epoch   6 |   400/  419 batches | ms/batch 56.20 | loss  5.52 | ppl   248.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 24.47s | valid loss  5.37 | valid ppl   214.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "LOG INTERVAL 100\n",
            "| epoch   7 |   100/  419 batches | ms/batch 56.86 | loss  5.56 | ppl   259.39\n",
            "LOG INTERVAL 100\n",
            "| epoch   7 |   200/  419 batches | ms/batch 56.33 | loss  5.48 | ppl   240.43\n",
            "LOG INTERVAL 100\n",
            "| epoch   7 |   300/  419 batches | ms/batch 56.30 | loss  5.45 | ppl   232.27\n",
            "LOG INTERVAL 100\n",
            "| epoch   7 |   400/  419 batches | ms/batch 56.27 | loss  5.42 | ppl   225.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 24.48s | valid loss  5.31 | valid ppl   201.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "LOG INTERVAL 100\n",
            "| epoch   8 |   100/  419 batches | ms/batch 56.89 | loss  5.47 | ppl   237.16\n",
            "LOG INTERVAL 100\n",
            "| epoch   8 |   200/  419 batches | ms/batch 56.29 | loss  5.40 | ppl   220.34\n",
            "LOG INTERVAL 100\n",
            "| epoch   8 |   300/  419 batches | ms/batch 56.22 | loss  5.36 | ppl   213.09\n",
            "LOG INTERVAL 100\n",
            "| epoch   8 |   400/  419 batches | ms/batch 56.42 | loss  5.33 | ppl   206.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 24.49s | valid loss  5.25 | valid ppl   190.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "LOG INTERVAL 100\n",
            "| epoch   9 |   100/  419 batches | ms/batch 57.03 | loss  5.39 | ppl   219.04\n",
            "LOG INTERVAL 100\n",
            "| epoch   9 |   200/  419 batches | ms/batch 56.40 | loss  5.32 | ppl   204.36\n",
            "LOG INTERVAL 100\n",
            "| epoch   9 |   300/  419 batches | ms/batch 56.32 | loss  5.29 | ppl   197.66\n",
            "LOG INTERVAL 100\n",
            "| epoch   9 |   400/  419 batches | ms/batch 56.36 | loss  5.26 | ppl   191.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 24.51s | valid loss  5.22 | valid ppl   184.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "LOG INTERVAL 100\n",
            "| epoch  10 |   100/  419 batches | ms/batch 56.98 | loss  5.31 | ppl   203.33\n",
            "LOG INTERVAL 100\n",
            "| epoch  10 |   200/  419 batches | ms/batch 56.45 | loss  5.25 | ppl   191.41\n",
            "LOG INTERVAL 100\n",
            "| epoch  10 |   300/  419 batches | ms/batch 56.44 | loss  5.22 | ppl   185.72\n",
            "LOG INTERVAL 100\n",
            "| epoch  10 |   400/  419 batches | ms/batch 56.41 | loss  5.19 | ppl   179.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 24.53s | valid loss  5.17 | valid ppl   176.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n"
          ]
        }
      ],
      "source": [
        "# prepare the model, loss, and optimizer\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = LSTMModel(ntokens, WORD_EMBED_DIM, HID_EMBED_DIM, N_LAYERS, DROPOUT, TIED).to(device)\n",
        "criterion = nn.CrossEntropyLoss() # use crossentropy loss\n",
        "optimizer = torch.optim.Adam(model.parameters()) # use adam optimizer with default setting\n",
        "best_val_loss = None\n",
        "\n",
        "# Training framework\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "        val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    \n",
        "    # Save the model if the validation loss is the best we've seen so far.\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        with open(SAVE_BEST, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "            print(\"save new best model!\")\n",
        "        best_val_loss = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "collapsed": true,
        "id": "1XxYhaWUicWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a53a7c3-bfec-4cff-97ba-0566d119959a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.10 | test ppl   163.91\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load the best saved model.\n",
        "with open(SAVE_BEST, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # After loading the RNN params, they are not a continuous chunk of memory.\n",
        "    # flatten_paramters() makes them a continuous chunk, and will speed up the forward pass.\n",
        "    # Currently, only RNN model supports flatten_parameters function.\n",
        "    model.lstm.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "mmDTj4Bg1lu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "YmzWgouuHhwF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "52688729886c46f5939fafbd90b5b466",
            "7341567feb1a4654ba3c6dacfa78dca8",
            "b96c34fd069f4f239499382849b3df9b",
            "e9cf338fa835456a9434166d0e096f9e",
            "94728a4883254b718d0fb280a159564e",
            "7f07d9e60dbc4621b0c5fd3ec25c042c",
            "10f55611e16c46749b66a6e4e8370df2",
            "15c3a47c4f794978a832fab868d51849",
            "5ede0859dc81474e9e82765834202bdb",
            "7a3a95e9774f4fc58061b8aa5127c043",
            "efd1b4f2b2744e2d86ce3de97e6cd746",
            "d3fb7e575f0d43ae9e3a93a4d01841d2",
            "92181e9984cb40f7a810812a87faaf29",
            "4a3cd9c0fbfc4f2091c89550b87c5cf0",
            "848856cab6594a52a4e6e3141df987a2",
            "8e6d60cc2f2145b0b4bd0f8254ed40c9",
            "a88cda9cc8f14560844a4311940e0302",
            "7a839012f64249b2b5674259d3a8b662",
            "7f3de92f682b4ce7af3faf02f6d2d9c8",
            "46486a82756240a9afc16f0fae1b6c0c",
            "9ed7d562a16340ef81564e132d30a77b",
            "82cef03bf7894d78891eaa127bd735ae",
            "73f9f6aa71e04469a99a33135c232897",
            "0227ebe8c4f042b883b83ec2ed70bd85",
            "41ffdf6a69ca40b49bb131ddc9d378ed",
            "a62f753ccaee4537a31e22e3d5c052f6",
            "e13f93ba832943469a253662a6d224a2",
            "b38da54b78a44d0e911b5b6562a76821",
            "24577292ed1046e1be03a29a4abdc19c",
            "87aa1856003249df824480c792e3ccf3",
            "abd988d6653747cba84179253002eec0",
            "cbf407c73f594136b77df5dfa3161171",
            "20fac1f5d76340cca4707109bbf52b05",
            "6938b343730448bdbb68da86fdd3a2ea",
            "166ad6799bde452dac38750c3b083639",
            "a4d8e76922f64567a26069948792653f",
            "65b97e9e208a4e2cbbeb9b0d802482ad",
            "170ae7f063ca4f66ab1a48224dd5fa30",
            "dcf10032d8d94892afee6f9662f33e1a",
            "a2fcdea5a4634dc7ad6988f228c14012",
            "fe5845f8a6a247768db00c6c55e38c1a",
            "0abfccd24aeb437e95f092d134ea6972",
            "952772b1418e48c0a6a3756c8c541e63",
            "cf1c60e6fb7349ba8f139df1d42e50d4"
          ]
        },
        "outputId": "ec42da6e-4df6-479d-c212-3ce916560f77"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52688729886c46f5939fafbd90b5b466",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3fb7e575f0d43ae9e3a93a4d01841d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73f9f6aa71e04469a99a33135c232897",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6938b343730448bdbb68da86fdd3a2ea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Generation with GPT-2\n",
        "# Check this tutorial: https://huggingface.co/blog/how-to-generate\n",
        "# It comes with a notebook. You need to run through that notebook and understand different sampling procedures.\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Search"
      ],
      "metadata": {
        "id": "7vvtpeIQ1_KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode('I went to', return_tensors='pt')\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "greedy_output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8ZLTyiA1am-",
        "outputId": "b1fe956d-ba0d-49f0-d04d-3f0374da99a6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to the hospital and was told that I was going to be taken to the hospital. I was told that I was going to be taken to the hospital and that I was going to be taken to the hospital. I was told that I was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ],
      "metadata": {
        "id": "Nz2bjxB23JD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activate beam search and early_stopping\n",
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hejL9ipj3Mwg",
        "outputId": "ae03dd9d-e869-4835-e1bc-2386f6723850"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to the doctor and said, 'I don't know what's going on. I don't know what's going on. I don't know what's going on. I don't know what's going on. I don't know what\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set no_repeat_ngram_size to 2\n",
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TJ073FP3h_i",
        "outputId": "b0b91360-e42b-4bc4-e225-8ae542b1286f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to the doctor and said, 'I don't know what's wrong with you, but I'm going to take care of you.'\"\n",
            "\n",
            "The doctor said he had no idea what was wrong. \"I didn't want to do anything\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling"
      ],
      "metadata": {
        "id": "efy_XY8Y3R_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RANDOM W/O TEMPERATURE\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KdWnLeR4Grr",
        "outputId": "b514c83c-bb3b-42be-f5a5-30e165b03e0a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to Spain to learn Spanish.\n",
            "\n",
            "How did you know Kristina was taking a class on birth control?\n",
            "\n",
            "We sat down. Ship parents talked about history and child development. We were set up to be indicative of people who didn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH TEMPERATURE\n",
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW2vx7554qH9",
        "outputId": "6de90296-1b0d-4658-ec35-66fee1e9d891"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to work, and one of my friends came into the office with a very wide towel. She said, \"You've got to go for your baby.\" I said, \"No.\" She came out and gave me the towel. Then she\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOP-K SAMPLING\n",
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioS8988g4_6f",
        "outputId": "ed682bf9-1335-4730-9e90-be9ea78f2577"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to college at New York University, and I spent my early-50s living in L.A. for two years to try to find work. I did some internships, and then I got married. I remember doing all of them\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOP-P\n",
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_p=0.92, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnfMD_Yf95gl",
        "outputId": "e025ca4c-297b-48c2-c027-0ea3887a1cb1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to a colleague at Stanford who was critical of our pace-fixing process. He was very vague about the technology. We are not cracking it yet,\" said Graham Watters, a Stanford Admissions professor who now has a Ph.D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "collapsed": true,
        "id": "aKmS7WvIHhwG"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, sampling_func):\n",
        "    # # Generation with LSTM lm given a sampling function and a prompt\n",
        "    max_length = 30\n",
        "    ids = []\n",
        "    for word in prompt.split():\n",
        "        ids.append(corpus.dictionary.word2idx[word])\n",
        "    hidden = model.init_hidden(1)\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        output, hidden = model(torch.LongTensor([[wid] for wid in ids]).to(device), hidden)\n",
        "        word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "        generations = []\n",
        "        for i in range(max_length):\n",
        "            word_idx = sampling_func(word_prob)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            generations.append(word)\n",
        "            if word == \"<eos>\":\n",
        "                break\n",
        "            new_word = torch.LongTensor([[word_idx]]).to(device)\n",
        "            output, hidden = model(new_word, hidden)\n",
        "            word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "    return generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "collapsed": true,
        "id": "MnEmUWiwHhwG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def greedy_sampling(word_prob):\n",
        "    # TODO: return the word with the max probability\n",
        "    word_id = torch.argmax(word_prob)\n",
        "    return word_id\n",
        "\n",
        "def random_sampling(word_prob):\n",
        "    # TODO: sample a random word based on the probability vector\n",
        "    word_id = random.randint(0, len(word_prob) - 1)\n",
        "    return word_id\n",
        "\n",
        "def topk_sampling(word_prob):\n",
        "    # TODO: top k sampling as explained in the assignment\n",
        "    indices = torch.sort(word_prob, descending = True)[:50][1]\n",
        "    index = random.randint(0, k - 1)\n",
        "    return indices[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": true,
        "id": "XpCGmlnTHhwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a43a43a-7816-430c-d5de-1feb85473eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i went to\n",
            "the <unk> \" . <eos>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"i went to\".lower()\n",
        "generations = generate_text(prompt, greedy_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generations = generate_text(prompt, random_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEzOF9nHcX5V",
        "outputId": "56dc4707-832c-4208-d352-3f23cc03ecfc"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i went to\n",
            "chagas exhaled transfers citizens palm pune conspirators bhai facial charity emigrated recruit imprison munro dispatched jeremi dubious writ trumerei rested antonescu northampton collision database stars shoemaker same exposes memoir boy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10\n",
        "generations = generate_text(prompt, topk_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDiJktaPc-yS",
        "outputId": "977d8abc-9e1a-4131-b449-03306562267e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i went to\n",
            "each the most story he made what at example he makes many back through a group in your . [ the female have an only and two characters like him\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 20\n",
        "generations = generate_text(prompt, topk_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM3GaC51c-XD",
        "outputId": "e5b5fa38-8c4c-42b3-e98f-11f8f90ced13"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i went to\n",
            "each years before they get off what we 'm actually get , because i am ' we may come against as for only being well , an few ways were\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 50\n",
        "generations = generate_text(prompt, topk_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojUEH_huchce",
        "outputId": "6ca3a4c6-1224-4f4a-a3ef-1ff57a05acd7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i went to\n",
            "that another of 20 knots may and get back all . although even in may 28 august 1940 after much the previous episode has only know by an least high\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'today at school'\n",
        "generations = generate_text(prompt, random_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejuzDnpNcofS",
        "outputId": "f3faa2f4-58d6-4a7e-ee12-7c364f21a70b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: today at school\n",
            "bed republican 1967 exacerbated outlaw cheke der harper trees genetics guthrie lawlessness nandi profitable ace curse santa gunshot inflation stylus fitzwarin amenable chests whoever sags pretend regionally exploration shows electronica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'it seems like'\n",
        "generations = generate_text(prompt, random_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN_UEauIdMPQ",
        "outputId": "e9d42cbb-3341-4bda-89b3-d62f7119dacf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: it seems like\n",
            "longitudinal curd delegates broods poke preparatory icp gabonica katzenjammer token indoctrination composition 28th lookouts bats ignorant rush variously doo tragic tripoli rajamouli forest merengue real airlines sensible sabo gather slovenes\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "a3_lm_template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52688729886c46f5939fafbd90b5b466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7341567feb1a4654ba3c6dacfa78dca8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b96c34fd069f4f239499382849b3df9b",
              "IPY_MODEL_e9cf338fa835456a9434166d0e096f9e",
              "IPY_MODEL_94728a4883254b718d0fb280a159564e"
            ]
          }
        },
        "7341567feb1a4654ba3c6dacfa78dca8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b96c34fd069f4f239499382849b3df9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f07d9e60dbc4621b0c5fd3ec25c042c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10f55611e16c46749b66a6e4e8370df2"
          }
        },
        "e9cf338fa835456a9434166d0e096f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_15c3a47c4f794978a832fab868d51849",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ede0859dc81474e9e82765834202bdb"
          }
        },
        "94728a4883254b718d0fb280a159564e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a3a95e9774f4fc58061b8aa5127c043",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.99M/0.99M [00:00&lt;00:00, 3.00MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efd1b4f2b2744e2d86ce3de97e6cd746"
          }
        },
        "7f07d9e60dbc4621b0c5fd3ec25c042c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10f55611e16c46749b66a6e4e8370df2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "15c3a47c4f794978a832fab868d51849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ede0859dc81474e9e82765834202bdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a3a95e9774f4fc58061b8aa5127c043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efd1b4f2b2744e2d86ce3de97e6cd746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3fb7e575f0d43ae9e3a93a4d01841d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_92181e9984cb40f7a810812a87faaf29",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4a3cd9c0fbfc4f2091c89550b87c5cf0",
              "IPY_MODEL_848856cab6594a52a4e6e3141df987a2",
              "IPY_MODEL_8e6d60cc2f2145b0b4bd0f8254ed40c9"
            ]
          }
        },
        "92181e9984cb40f7a810812a87faaf29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a3cd9c0fbfc4f2091c89550b87c5cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a88cda9cc8f14560844a4311940e0302",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a839012f64249b2b5674259d3a8b662"
          }
        },
        "848856cab6594a52a4e6e3141df987a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7f3de92f682b4ce7af3faf02f6d2d9c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46486a82756240a9afc16f0fae1b6c0c"
          }
        },
        "8e6d60cc2f2145b0b4bd0f8254ed40c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ed7d562a16340ef81564e132d30a77b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:00&lt;00:00, 898kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_82cef03bf7894d78891eaa127bd735ae"
          }
        },
        "a88cda9cc8f14560844a4311940e0302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a839012f64249b2b5674259d3a8b662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f3de92f682b4ce7af3faf02f6d2d9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46486a82756240a9afc16f0fae1b6c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ed7d562a16340ef81564e132d30a77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "82cef03bf7894d78891eaa127bd735ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73f9f6aa71e04469a99a33135c232897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0227ebe8c4f042b883b83ec2ed70bd85",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_41ffdf6a69ca40b49bb131ddc9d378ed",
              "IPY_MODEL_a62f753ccaee4537a31e22e3d5c052f6",
              "IPY_MODEL_e13f93ba832943469a253662a6d224a2"
            ]
          }
        },
        "0227ebe8c4f042b883b83ec2ed70bd85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41ffdf6a69ca40b49bb131ddc9d378ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b38da54b78a44d0e911b5b6562a76821",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24577292ed1046e1be03a29a4abdc19c"
          }
        },
        "a62f753ccaee4537a31e22e3d5c052f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_87aa1856003249df824480c792e3ccf3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 665,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_abd988d6653747cba84179253002eec0"
          }
        },
        "e13f93ba832943469a253662a6d224a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cbf407c73f594136b77df5dfa3161171",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 665/665 [00:00&lt;00:00, 26.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_20fac1f5d76340cca4707109bbf52b05"
          }
        },
        "b38da54b78a44d0e911b5b6562a76821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24577292ed1046e1be03a29a4abdc19c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87aa1856003249df824480c792e3ccf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "abd988d6653747cba84179253002eec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbf407c73f594136b77df5dfa3161171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "20fac1f5d76340cca4707109bbf52b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6938b343730448bdbb68da86fdd3a2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_166ad6799bde452dac38750c3b083639",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a4d8e76922f64567a26069948792653f",
              "IPY_MODEL_65b97e9e208a4e2cbbeb9b0d802482ad",
              "IPY_MODEL_170ae7f063ca4f66ab1a48224dd5fa30"
            ]
          }
        },
        "166ad6799bde452dac38750c3b083639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4d8e76922f64567a26069948792653f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dcf10032d8d94892afee6f9662f33e1a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2fcdea5a4634dc7ad6988f228c14012"
          }
        },
        "65b97e9e208a4e2cbbeb9b0d802482ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fe5845f8a6a247768db00c6c55e38c1a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0abfccd24aeb437e95f092d134ea6972"
          }
        },
        "170ae7f063ca4f66ab1a48224dd5fa30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_952772b1418e48c0a6a3756c8c541e63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 523M/523M [00:09&lt;00:00, 59.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf1c60e6fb7349ba8f139df1d42e50d4"
          }
        },
        "dcf10032d8d94892afee6f9662f33e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2fcdea5a4634dc7ad6988f228c14012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe5845f8a6a247768db00c6c55e38c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0abfccd24aeb437e95f092d134ea6972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "952772b1418e48c0a6a3756c8c541e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf1c60e6fb7349ba8f139df1d42e50d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}